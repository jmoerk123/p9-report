\begin{thebibliography}{}

\bibitem[Ba et~al., 2016]{ba2016layer}
Ba, J.~L., Kiros, J.~R., and Hinton, G.~E. (2016).
\newblock Layer normalization.
\newblock {\em arXiv preprint arXiv:1607.06450}.

\bibitem[Baevski et~al., 2022]{baevski2022data2vec}
Baevski, A., Hsu, W.-N., Xu, Q., Babu, A., Gu, J., and Auli, M. (2022).
\newblock Data2vec: A general framework for self-supervised learning in speech,
  vision and language.
\newblock In {\em International Conference on Machine Learning}, pages
  1298--1312. PMLR.

\bibitem[Balestriero et~al., 2023]{balestriero2023cookbook}
Balestriero, R., Ibrahim, M., Sobal, V., Morcos, A., Shekhar, S., Goldstein,
  T., Bordes, F., Bardes, A., Mialon, G., Tian, Y., et~al. (2023).
\newblock A cookbook of self-supervised learning.
\newblock {\em arXiv preprint arXiv:2304.12210}.

\bibitem[Barker et~al., 2015]{barker2015third}
Barker, J., Marxer, R., Vincent, E., and Watanabe, S. (2015).
\newblock The third 'chime'speech separation and recognition challenge:
  Dataset, task and baselines.
\newblock In {\em 2015 IEEE Workshop on Automatic Speech Recognition and
  Understanding (ASRU)}, pages 504--511. IEEE.

\bibitem[Berg et~al., 2021]{berg2021keyword}
Berg, A., O'Connor, M., and Cruz, M.~T. (2021).
\newblock Keyword transformer: A self-attention model for keyword spotting.
\newblock {\em arXiv preprint arXiv:2104.00769}.

\bibitem[Bovbjerg and Tan, 2023]{bovbjerg2023improving}
Bovbjerg, H.~S. and Tan, Z.-H. (2023).
\newblock Improving label-deficient keyword spotting through self-supervised
  pretraining.
\newblock In {\em 2023 IEEE International Conference on Acoustics, Speech, and
  Signal Processing Workshops (ICASSPW)}, pages 1--5. IEEE.

\bibitem[Dosovitskiy et~al., 2020]{dosovitskiy2020image}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
  (2020).
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}.

\bibitem[Kolboek et~al., 2016]{kolboek2016speech}
Kolboek, M., Tan, Z.-H., and Jensen, J. (2016).
\newblock Speech enhancement using long short-term memory based recurrent
  neural networks for noise robust speaker verification.
\newblock In {\em 2016 IEEE spoken language technology workshop (SLT)}, pages
  305--311. IEEE.

\bibitem[L{\'o}pez-Espejo et~al., 2021]{lopez2021deep}
L{\'o}pez-Espejo, I., Tan, Z.-H., Hansen, J.~H., and Jensen, J. (2021).
\newblock Deep spoken keyword spotting: An overview.
\newblock {\em IEEE Access}, 10:4169--4199.

\bibitem[Loshchilov and Hutter, 2018]{loshchilov2018fixing}
Loshchilov, I. and Hutter, F. (2018).
\newblock Fixing weight decay regularization in adam.

\bibitem[Park et~al., 2019]{park2019specaugment}
Park, D.~S., Chan, W., Zhang, Y., Chiu, C.-C., Zoph, B., Cubuk, E.~D., and Le,
  Q.~V. (2019).
\newblock Specaugment: A simple data augmentation method for automatic speech
  recognition.
\newblock {\em arXiv preprint arXiv:1904.08779}.

\bibitem[Rothman, 2020]{rothman2020artificial}
Rothman, D. (2020).
\newblock {\em Artificial Intelligence By Example: Acquire advanced AI, machine
  learning, and deep learning design skills}.
\newblock Packt Publishing Ltd.

\bibitem[Ulyanov et~al., 2016]{ulyanov2016instance}
Ulyanov, D., Vedaldi, A., and Lempitsky, V. (2016).
\newblock Instance normalization: The missing ingredient for fast stylization.
\newblock {\em arXiv preprint arXiv:1607.08022}.

\bibitem[Vaswani et~al., 2017]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I. (2017).
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30.

\bibitem[Warden, 2018]{warden2018speech}
Warden, P. (2018).
\newblock Speech commands: A dataset for limited-vocabulary speech recognition.
\newblock {\em arXiv preprint arXiv:1804.03209}.

\end{thebibliography}
