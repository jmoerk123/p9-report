In this chapter the experiment will be outlined, whereafter the results of the experiment will be presented. The experiment includes five different models: two baseline, and three test models.

\section{Data}
The data used in this project is the Google Speech Commands V2 dataset \cite{warden2018speech}, which consists of \(105829\) one second recordings of \(35\) different keywords, outlined in \Cref{tab:google_words}. The dataset has been manipulated such that \(80\%\) of the training data are without labels. The \(80\%\) will be used to pretrain the models, and the remaining \(20\%\) will then be used to finetune the models and to train a baseline model. The final spilt of the recordings can be seen in \Cref{tab:data_split}

\begin{table}[ht]
    \centering
    \begin{tabular}{@{}lcccc@{}}
        \toprule
        Use    & Pretraining & \makecell{ finetuning/ \\ Baseline } & Validating & Testing \\ \midrule
        Recordings   & 67874  & 16969 & 9981 & 11005 \\ 
        \bottomrule
    \end{tabular}
    \caption{How the 105820 recordings are split}
    \label{tab:data_split}
\end{table}

Four different types of noise are used in the experiment, which can be put into two categories, one being stationary an another being non-stationary. The two types of noises in the stationary category are bus (BUS) and cafe (CAF), which have been used by \cite{barker2015third}. The other two types of noises are babble (BBL) and speech shaped noise (SSN), which were generated by \cite{kolboek2016speech}. The noises have been added to the keywords in \(7\) different SNR's: \(-10, \ -5, \ 0, \ 5, \ 10, \ 15, \ 20\). This yields \(28\) datasets, one for each noise with each SNR.
For each SNR two mixed data sets have been created, one is a \(50/50\) mixture of keywords with BUS and BBL noise. Another mixed dataset is a equal mixture of keywords with all types of noise.

\section{Model Setup}
The training and testing is using the same setup and models as in \cite{bovbjerg2023improving}. Here a keyword transformer (KWT) \cite{berg2021keyword}, with a added mean of the encodings of each time step, is used. 
%The KWT model is based on a vision transformer \cite{dosovitskiy2020image}, where the image is substituted with Mel-frequency cepstral coefficients (MFCC), which are explained in \Cref{ch:theory}, and have achieved state-of-the-art performance on the Google Speech Commands KWS benchmark.

Following \cite{bovbjerg2023improving}, three versions of the KWT is used. Varying the transformers amount of attention heads from one to three and the encoder dimension from \(64\) to \(192\) yields the three models: KWT-1, KWT-2, and KWT-3, with \(0.6\cdot 10^6\), \(2.4\cdot 10^6\), and \(5.4 \cdot 10^6\) parameters respectively \cite{bovbjerg2023improving}. 

In order to validate the noise-robustness of the SSL model two baseline models are trained. One being the baseline-supervised, which is a fully supervised model trained on the \(20\%\) labeled data. 
The second baseline model is a model pretrained and finetuned using noiseless data, this model is referred to as the baseline-Data2vec model. The three test models are all finetuned on the same data as the baseline-supervised is trained on and pretrained on different data. One is trained on noiseless data, and another is trained on noisy data, these are referred to as pretrained-noiseless and pretrained-noisy respectively. The last test model is trained by feeding to the student noisy data and the teacher with noiseless data, this model is referred to as the pretrained-both model. 

Training the five models three times, one with each KWT results in \(15\) models in total.


\subsection{Model Training}
Using the same setup as in \cite{bovbjerg2023improving} the finetuning and baseline-supervised model is trained for \(140\) epochs with a batch size of \(512\), using cross entropy as the learning objective. The weights are updated using the AdamW \cite{loshchilov2018fixing} optimizer with a learning rate of \(1 \cdot 10^ {-3}\) and a weight decay of \(0.1\), where the learning rate schedular used is a \(10\) epochs linear warmup followed by cosine annealing. Furthermore, doing the training of the model SpecAugment \cite{park2019specaugment} is randomly applied to mask blocks in both time and feature dimension \cite{bovbjerg2023improving}. 

For the pretraining the time domain masking strategy is the same as used in Wav2Vec \cite{}. Here a MFCC vector is picked with probability of \(0.65\) and the next \(10\) MFCC vectors are then replaced by a mask token embedding. The hyperparameters used are the same as the ones used in \cite{bovbjerg2023improving}, where most of the hyperparameters have been chosen according to the original Data2vec study \cite{baevski2022data2vec}, with a few changes due to different in data and hardware.

The models have been train using the implementation created by the author of \cite{bovbjerg2023improving}. 

\section{Test Setup}
The experiment consists of two tests. In the first test the same kind of noise as in the training (i.e., BUS and BBL) is added, but using only one SNR. The accuracy of the \(15\) models are then found for each SNR. The results of this test can be found in \Cref{subsec:test1}.

The second test is performed using the four noise types (i.e., BUS, BBL, CAF, and SSN). The results of this test can be found in \Cref{subsec:test2}.


%For these tests \(63\) models have been trained. A KWT-1, a KWT-2, and a KWT-3 model have been trained using both the baseline method, the pretraining on clean data, and pretraining on noisy data, for each SNR. The noise added to the training data is af \(50/50\) mix of CAF and BBL.

%The first test is testing the models on data where the same kind of noise as used in the training have been added, i.e., CAF and BBL. For the second test 