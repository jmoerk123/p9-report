In this chapter the experiment will be outlined, whereafter the results of the experiment will be presented. The experiment includes five different models: two baseline models, and three test models.

\section{Data}
The data used in this project is the Google Speech Commands V2 dataset \cite{warden2018speech}, which consists of \(105829\) one second recordings of \(35\) different keywords, outlined in \Cref{tab:google_words}. The dataset has been manipulated such that \(80\%\) of the training data are without labels. The \(80\%\) will be used to pretrain the models, and the remaining \(20\%\) will then be used to finetune the models and to train a baseline model. The final spilt of the recordings can be seen in \Cref{tab:data_split}

\begin{table}[ht]
    \centering
    \begin{tabular}{@{}lcccc@{}}
        \toprule
        Use    & Pretraining & \makecell{ finetuning/ \\ Baseline } & Validating & Testing \\ \midrule
        Recordings   & 67874  & 16969 & 9981 & 11005 \\ 
        \bottomrule
    \end{tabular}
    \caption{How the 105820 recordings are split}
    \label{tab:data_split}
\end{table}

Four different types of noise are used in the experiment, which can be put into two categories, one being stationary an another being non-stationary. The two types of noises in the stationary category are bus (BUS) and cafe (CAF), which have been used by \cite{barker2015third}. The other two types of noises are babble (BBL) and speech shaped noise (SSN), which were generated by \cite{kolboek2016speech}. The noises have been added to the keywords in \(7\) different SNR's: \(-10, \ -5, \ 0, \ 5, \ 10, \ 15, \ 20\). This yields \(28\) datasets, one for each noise with each SNR.
For each SNR two mixed data sets have been created, one is a \(50/50\) mixture of keywords with BUS and BBL noise. Another mixed dataset is a equal mixture of keywords with the four types of noise.

\section{Model Setup}
The training and testing is using the same setup and models as in \cite{bovbjerg2023improving}. Here a keyword transformer (KWT) \cite{berg2021keyword}, with a added mean of the encodings of each time step, is used. 
%The KWT model is based on a vision transformer \cite{dosovitskiy2020image}, where the image is substituted with Mel-frequency cepstral coefficients (MFCC), which are explained in \Cref{ch:theory}, and have achieved state-of-the-art performance on the Google Speech Commands KWS benchmark.

Following \cite{bovbjerg2023improving}, three versions of the KWT is used. Varying the transformers amount of attention heads from one to three and the encoder dimension from \(64\) to \(192\) yields the three models: KWT-1, KWT-2, and KWT-3, with \(0.6\cdot 10^6\), \(2.4\cdot 10^6\), and \(5.4 \cdot 10^6\) parameters respectively \cite{bovbjerg2023improving}. 

In order to validate the noise-robustness of the SSL models two baseline models are trained. One being the baseline-supervised, which is a fully supervised model trained on the \(20\%\) labeled data. 
The second baseline model is a model pretrained and finetuned using noiseless data, this model is referred to as the baseline-Data2vec model. The three test models are all finetuned on the same data as the baseline-supervised is trained on and pretrained on different data. One is trained on noiseless data, and another is trained on noisy data, these are referred to as pretrained-noiseless and pretrained-noisy respectively. The last test model is trained by feeding the student noisy data and the teacher noiseless data, this model is referred to as the pretrained-both model. 

Training the five models three times, one with each KWT results in \(15\) models in total.


\subsection{Model Training}
Using the same setup as in \cite{bovbjerg2023improving} the finetuning and baseline-supervised model is trained for \(140\) epochs with a batch size of \(512\), using cross entropy as the learning objective. The weights are updated using the AdamW \cite{loshchilov2018fixing} optimizer with a learning rate of \(1 \cdot 10^ {-3}\) and a weight decay of \(0.1\), where the learning rate schedular used is a \(10\) epochs linear warmup followed by cosine annealing. Furthermore, doing the training of the model SpecAugment \cite{park2019specaugment} is randomly applied to mask blocks in both time and feature dimension \cite{bovbjerg2023improving}. 

For the pretraining the time domain masking strategy is the same as used in Wav2Vec \cite{}. Here a MFCC vector is picked with probability of \(0.65\) and the next \(10\) MFCC vectors are then replaced by a mask token embedding. The hyperparameters used are the same as the ones used in \cite{bovbjerg2023improving}, where most of the hyperparameters have been chosen according to the original Data2vec study \cite{baevski2022data2vec}, with a few changes due to different in data and hardware.

The noisy data used to train the models is the data where only BUS and BBL noise is applied. This entails that non of the models have any experience with the noise types CAF or SNN.

The models have been trained using the implementation created by the author of \cite{bovbjerg2023improving}. 

\section{Test Setup}
To validate the models, the accuracy of the model , when testing them on new sound files which they have never seen before, is calculated. This is done using 
\begin{align} \label{eq:acc}
    acc = \frac{TP + TN}{TP + TN + FP + FN},
\end{align}
on each keyword and average the results.

Two test are conducted, one where the the noise added to the sound files are the same kind of noise which is added to the training data (i.e., BUS and BBL). The results of this test can be found in \Cref{subsec:test1}. The second test is performed using the four noise types (i.e., BUS, BBL, CAF, and SSN). The results of this test can be found in \Cref{subsec:test2}.
